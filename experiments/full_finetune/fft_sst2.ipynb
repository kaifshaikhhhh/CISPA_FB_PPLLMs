{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "from opacus.validators import ModuleValidator\n",
    "from opacus.utils.batch_memory_manager import BatchMemoryManager\n",
    "from opacus import PrivacyEngine\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.optim import SGD\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoConfig\n",
    "\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"prajjwal1/bert-tiny\"\n",
    "EPOCHS = 4\n",
    "BATCH_SIZE = 32\n",
    "LR = 2e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "dataset = load_dataset(\"glue\", \"sst2\")\n",
    "num_labels = dataset[\"train\"].features[\"label\"].num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = dataset.map(\n",
    "    lambda example: tokenizer(example[\"sentence\"], max_length=128, padding='max_length', truncation=True),\n",
    "    batched=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "tokenized_dataset = tokenized_dataset.remove_columns(['idx'])\n",
    "tokenized_dataset = tokenized_dataset.rename_column(\"label\", \"labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(tokenized_dataset[\"train\"], shuffle=False, batch_size=BATCH_SIZE)\n",
    "test_dataloader = DataLoader(tokenized_dataset[\"validation\"], shuffle=False, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPSILON = 8.0\n",
    "DELTA = 1/len(train_dataloader)\n",
    "MAX_GRAD_NORM = 0.01\n",
    "MAX_PHYSICAL_BATCH_SIZE = int(BATCH_SIZE/4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 128, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 128)\n",
       "      (token_type_embeddings): Embedding(2, 128)\n",
       "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-1): 2 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=128, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained(model_name)\n",
    "config.num_labels = num_labels\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = SGD(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "privacy_engine = PrivacyEngine(accountant=\"rdp\")\n",
    "\n",
    "model, optimizer, train_dataset = privacy_engine.make_private_with_epsilon(\n",
    "    module=model,\n",
    "    optimizer=optimizer,\n",
    "    data_loader=train_dataloader,\n",
    "    epochs=EPOCHS,\n",
    "    target_epsilon=EPSILON,\n",
    "    target_delta=DELTA,\n",
    "    max_grad_norm=MAX_GRAD_NORM,\n",
    "    batch_first=True,    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Sigma = 0.382 | C = 0.01 | Initial DP (ε, δ) = (0, 0.00047505938242280285)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Using Sigma = {optimizer.noise_multiplier:.3f} | C = {optimizer.max_grad_norm} | Initial DP (ε, δ) = ({privacy_engine.get_epsilon(DELTA)}, {DELTA})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable Parameters: 4386178 || All Parameters: 4386178 || Trainable Parameters (%): 100.00\n"
     ]
    }
   ],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"Trainable Parameters: {trainable_params} || All Parameters: {all_param} || Trainable Parameters (%): {100 * trainable_params / all_param:.2f}\"\n",
    "    )\n",
    "\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dataloader, optimizer, epoch, device):\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    for i, batch in tqdm(enumerate(train_dataloader), total=len(train_dataloader), desc=f\"Training Epoch: {epoch}\"):\n",
    "        \n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(**batch)\n",
    "        loss = criterion(outputs.logits, batch[\"labels\"])\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        if i % 8000 == 0:\n",
    "            epsilon = privacy_engine.get_epsilon(DELTA)\n",
    "\n",
    "            print(f\"Training Epoch: {epoch} | Loss: {np.mean(losses):.6f} | ε = {0:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e695d0e83064fa2a230f90b5f5f91a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bbe81961c6649c5b2e765d6f206b869",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch: 4:   0%|          | 0/2105 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [32] at entry 0 and [1] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32me:\\Internships\\Applications & Cover Letters\\CISPA Helmhotz\\CISPA_FB_PPLLMs\\experiments\\full_finetune\\fft_sst2.ipynb Cell 18\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Internships/Applications%20%26%20Cover%20Letters/CISPA%20Helmhotz/CISPA_FB_PPLLMs/experiments/full_finetune/fft_sst2.ipynb#X24sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m tqdm_epoch \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, EPOCHS\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m), desc\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTraining\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/Internships/Applications%20%26%20Cover%20Letters/CISPA%20Helmhotz/CISPA_FB_PPLLMs/experiments/full_finetune/fft_sst2.ipynb#X24sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     train(model, train_dataloader, optimizer, EPOCHS, device)\n",
      "\u001b[1;32me:\\Internships\\Applications & Cover Letters\\CISPA Helmhotz\\CISPA_FB_PPLLMs\\experiments\\full_finetune\\fft_sst2.ipynb Cell 18\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Internships/Applications%20%26%20Cover%20Letters/CISPA%20Helmhotz/CISPA_FB_PPLLMs/experiments/full_finetune/fft_sst2.ipynb#X24sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs\u001b[39m.\u001b[39mlogits, batch[\u001b[39m\"\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Internships/Applications%20%26%20Cover%20Letters/CISPA%20Helmhotz/CISPA_FB_PPLLMs/experiments/full_finetune/fft_sst2.ipynb#X24sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/Internships/Applications%20%26%20Cover%20Letters/CISPA%20Helmhotz/CISPA_FB_PPLLMs/experiments/full_finetune/fft_sst2.ipynb#X24sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Internships/Applications%20%26%20Cover%20Letters/CISPA%20Helmhotz/CISPA_FB_PPLLMs/experiments/full_finetune/fft_sst2.ipynb#X24sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m losses\u001b[39m.\u001b[39mappend(loss\u001b[39m.\u001b[39mitem())\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Internships/Applications%20%26%20Cover%20Letters/CISPA%20Helmhotz/CISPA_FB_PPLLMs/experiments/full_finetune/fft_sst2.ipynb#X24sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mif\u001b[39;00m i \u001b[39m%\u001b[39m \u001b[39m8000\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\KXIF\\anaconda3\\envs\\workenv\\lib\\site-packages\\opacus\\optimizers\\optimizer.py:513\u001b[0m, in \u001b[0;36mDPOptimizer.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    510\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39menable_grad():\n\u001b[0;32m    511\u001b[0m         closure()\n\u001b[1;32m--> 513\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpre_step():\n\u001b[0;32m    514\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moriginal_optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m    515\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\KXIF\\anaconda3\\envs\\workenv\\lib\\site-packages\\opacus\\optimizers\\optimizer.py:494\u001b[0m, in \u001b[0;36mDPOptimizer.pre_step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    483\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpre_step\u001b[39m(\n\u001b[0;32m    484\u001b[0m     \u001b[39mself\u001b[39m, closure: Optional[Callable[[], \u001b[39mfloat\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    485\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Optional[\u001b[39mfloat\u001b[39m]:\n\u001b[0;32m    486\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    487\u001b[0m \u001b[39m    Perform actions specific to ``DPOptimizer`` before calling\u001b[39;00m\n\u001b[0;32m    488\u001b[0m \u001b[39m    underlying  ``optimizer.step()``\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    492\u001b[0m \u001b[39m            returns the loss. Optional for most optimizers.\u001b[39;00m\n\u001b[0;32m    493\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 494\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclip_and_accumulate()\n\u001b[0;32m    495\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_skip_next_step():\n\u001b[0;32m    496\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_last_step_skipped \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\KXIF\\anaconda3\\envs\\workenv\\lib\\site-packages\\opacus\\optimizers\\optimizer.py:404\u001b[0m, in \u001b[0;36mDPOptimizer.clip_and_accumulate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    400\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    401\u001b[0m     per_param_norms \u001b[39m=\u001b[39m [\n\u001b[0;32m    402\u001b[0m         g\u001b[39m.\u001b[39mreshape(\u001b[39mlen\u001b[39m(g), \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mnorm(\u001b[39m2\u001b[39m, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39mfor\u001b[39;00m g \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgrad_samples\n\u001b[0;32m    403\u001b[0m     ]\n\u001b[1;32m--> 404\u001b[0m     per_sample_norms \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mstack(per_param_norms, dim\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\u001b[39m.\u001b[39mnorm(\u001b[39m2\u001b[39m, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m    405\u001b[0m     per_sample_clip_factor \u001b[39m=\u001b[39m (\n\u001b[0;32m    406\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_grad_norm \u001b[39m/\u001b[39m (per_sample_norms \u001b[39m+\u001b[39m \u001b[39m1e-6\u001b[39m)\n\u001b[0;32m    407\u001b[0m     )\u001b[39m.\u001b[39mclamp(\u001b[39mmax\u001b[39m\u001b[39m=\u001b[39m\u001b[39m1.0\u001b[39m)\n\u001b[0;32m    409\u001b[0m \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparams:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [32] at entry 0 and [1] at entry 1"
     ]
    }
   ],
   "source": [
    "for tqdm_epoch in tqdm(range(1, EPOCHS+1), desc=\"Training\"):\n",
    "    train(model, train_dataloader, optimizer, EPOCHS, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.eval()\n",
    "\n",
    "predictions = []\n",
    "labels = []\n",
    "\n",
    "for batch in tqdm(train_dataloader, total=len(train_dataloader), desc=\"Testing\"):\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "        predictions.append(outputs.logits.cpu().numpy())\n",
    "        labels.append(batch[\"labels\"].cpu().numpy())\n",
    "\n",
    "predictions = np.concatenate(predictions)\n",
    "labels = np.concatenate(labels)\n",
    "\n",
    "print(f\"Accuracy: {flat_accuracy(predictions, labels)*100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
