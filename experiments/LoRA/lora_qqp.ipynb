{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-15 09:09:02.212346: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-15 09:09:02.501720: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-11-15 09:09:02.507985: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.2/lib64:/opt/software/Java/1.8.0_152/lib:/opt/software/Python/3.6.4-foss-2018a/lib:/opt/software/libffi/3.2.1-GCCcore-6.4.0/lib64:/opt/software/libffi/3.2.1-GCCcore-6.4.0/lib:/opt/software/GMP/6.1.2-GCCcore-6.4.0/lib:/opt/software/SQLite/3.21.0-GCCcore-6.4.0/lib:/opt/software/Tcl/8.6.8-GCCcore-6.4.0/lib:/opt/software/libreadline/7.0-GCCcore-6.4.0/lib:/opt/software/ncurses/6.0-GCCcore-6.4.0/lib:/opt/software/Boost/1.67.0-foss-2018a/lib:/opt/software/zlib/1.2.11-GCCcore-6.4.0/lib:/opt/software/bzip2/1.0.6-GCCcore-6.4.0/lib:/opt/software/ScaLAPACK/2.0.2-gompi-2018a-OpenBLAS-0.2.20/lib:/opt/software/FFTW/3.3.7-gompi-2018a/lib:/opt/software/OpenBLAS/0.2.20-GCC-6.4.0-2.28/lib:/opt/software/imkl/2018.1.163-gompi-2018a/mkl/lib/intel64:/opt/software/imkl/2018.1.163-gompi-2018a/lib/intel64:/opt/software/tbb/2018_U3-GCCcore-6.4.0/build/linux_intel64_gcc_cc6.4.0_libc2.17_kernel3.10.0_release:/opt/software/OpenMPI/2.1.2-GCC-6.4.0-2.28/lib:/opt/software/hwloc/1.11.8-GCCcore-6.4.0/lib:/opt/software/numactl/2.0.11-GCCcore-6.4.0/lib:/opt/software/binutils/2.28-GCCcore-6.4.0/lib:/opt/software/GCCcore/6.4.0/lib/gcc/x86_64-pc-linux-gnu/6.4.0:/opt/software/GCCcore/6.4.0/lib64:/opt/software/GCCcore/6.4.0/lib:/usr/local/cuda-11.2/lib64\n",
      "2023-11-15 09:09:02.508005: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-11-15 09:09:04.337548: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.2/lib64:/opt/software/Java/1.8.0_152/lib:/opt/software/Python/3.6.4-foss-2018a/lib:/opt/software/libffi/3.2.1-GCCcore-6.4.0/lib64:/opt/software/libffi/3.2.1-GCCcore-6.4.0/lib:/opt/software/GMP/6.1.2-GCCcore-6.4.0/lib:/opt/software/SQLite/3.21.0-GCCcore-6.4.0/lib:/opt/software/Tcl/8.6.8-GCCcore-6.4.0/lib:/opt/software/libreadline/7.0-GCCcore-6.4.0/lib:/opt/software/ncurses/6.0-GCCcore-6.4.0/lib:/opt/software/Boost/1.67.0-foss-2018a/lib:/opt/software/zlib/1.2.11-GCCcore-6.4.0/lib:/opt/software/bzip2/1.0.6-GCCcore-6.4.0/lib:/opt/software/ScaLAPACK/2.0.2-gompi-2018a-OpenBLAS-0.2.20/lib:/opt/software/FFTW/3.3.7-gompi-2018a/lib:/opt/software/OpenBLAS/0.2.20-GCC-6.4.0-2.28/lib:/opt/software/imkl/2018.1.163-gompi-2018a/mkl/lib/intel64:/opt/software/imkl/2018.1.163-gompi-2018a/lib/intel64:/opt/software/tbb/2018_U3-GCCcore-6.4.0/build/linux_intel64_gcc_cc6.4.0_libc2.17_kernel3.10.0_release:/opt/software/OpenMPI/2.1.2-GCC-6.4.0-2.28/lib:/opt/software/hwloc/1.11.8-GCCcore-6.4.0/lib:/opt/software/numactl/2.0.11-GCCcore-6.4.0/lib:/opt/software/binutils/2.28-GCCcore-6.4.0/lib:/opt/software/GCCcore/6.4.0/lib/gcc/x86_64-pc-linux-gnu/6.4.0:/opt/software/GCCcore/6.4.0/lib64:/opt/software/GCCcore/6.4.0/lib:/usr/local/cuda-11.2/lib64\n",
      "2023-11-15 09:09:04.338678: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.2/lib64:/opt/software/Java/1.8.0_152/lib:/opt/software/Python/3.6.4-foss-2018a/lib:/opt/software/libffi/3.2.1-GCCcore-6.4.0/lib64:/opt/software/libffi/3.2.1-GCCcore-6.4.0/lib:/opt/software/GMP/6.1.2-GCCcore-6.4.0/lib:/opt/software/SQLite/3.21.0-GCCcore-6.4.0/lib:/opt/software/Tcl/8.6.8-GCCcore-6.4.0/lib:/opt/software/libreadline/7.0-GCCcore-6.4.0/lib:/opt/software/ncurses/6.0-GCCcore-6.4.0/lib:/opt/software/Boost/1.67.0-foss-2018a/lib:/opt/software/zlib/1.2.11-GCCcore-6.4.0/lib:/opt/software/bzip2/1.0.6-GCCcore-6.4.0/lib:/opt/software/ScaLAPACK/2.0.2-gompi-2018a-OpenBLAS-0.2.20/lib:/opt/software/FFTW/3.3.7-gompi-2018a/lib:/opt/software/OpenBLAS/0.2.20-GCC-6.4.0-2.28/lib:/opt/software/imkl/2018.1.163-gompi-2018a/mkl/lib/intel64:/opt/software/imkl/2018.1.163-gompi-2018a/lib/intel64:/opt/software/tbb/2018_U3-GCCcore-6.4.0/build/linux_intel64_gcc_cc6.4.0_libc2.17_kernel3.10.0_release:/opt/software/OpenMPI/2.1.2-GCC-6.4.0-2.28/lib:/opt/software/hwloc/1.11.8-GCCcore-6.4.0/lib:/opt/software/numactl/2.0.11-GCCcore-6.4.0/lib:/opt/software/binutils/2.28-GCCcore-6.4.0/lib:/opt/software/GCCcore/6.4.0/lib/gcc/x86_64-pc-linux-gnu/6.4.0:/opt/software/GCCcore/6.4.0/lib64:/opt/software/GCCcore/6.4.0/lib:/usr/local/cuda-11.2/lib64\n",
      "2023-11-15 09:09:04.338691: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "\n",
    "import opacus\n",
    "from opacus.validators import ModuleValidator\n",
    "from opacus.utils.batch_memory_manager import BatchMemoryManager\n",
    "from opacus import PrivacyEngine\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.optim import SGD\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoConfig\n",
    "\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"prajjwal1/bert-tiny\"\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 2048\n",
    "LR = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebac8bce808b405a9f81a3e8cdcb77a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/41.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55c28e4b5cb1469f9ae1d2442bb251e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/363846 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc8eebf40fad4f62893010a1ddbad0d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/40430 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e21084bdd27f42bc8148e36c70d06ef1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/390965 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Prepare data\n",
    "dataset = load_dataset(\"glue\", \"qqp\")\n",
    "num_labels = dataset[\"train\"].features[\"label\"].num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e027bcea9af4bd6ac339be9c493a33f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/363846 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40781ed9b70f485bbe47ca860aecdc29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/40430 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "456eeaf6515f484fa80d2f83fe9111cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/390965 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(\n",
    "    lambda example: tokenizer(\n",
    "        example[\"question1\"],\n",
    "        example[\"question2\"],\n",
    "        max_length=128,\n",
    "        padding='max_length',\n",
    "        truncation=True\n",
    "    ),\n",
    "    batched=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "tokenized_dataset = tokenized_dataset.remove_columns(['idx'])\n",
    "tokenized_dataset = tokenized_dataset.rename_column(\"label\", \"labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question1', 'question2', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 363846\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['question1', 'question2', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 40430\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question1', 'question2', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 390965\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(tokenized_dataset[\"train\"], shuffle=False, batch_size=BATCH_SIZE)\n",
    "test_dataloader = DataLoader(tokenized_dataset[\"validation\"], shuffle=False, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPSILON = 8.0\n",
    "DELTA = 1/len(train_dataloader)\n",
    "MAX_GRAD_NORM = 0.5\n",
    "MAX_PHYSICAL_BATCH_SIZE = int(BATCH_SIZE/4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForSequenceClassification(\n",
       "  (base_model): LoraModel(\n",
       "    (model): BertForSequenceClassification(\n",
       "      (bert): BertModel(\n",
       "        (embeddings): BertEmbeddings(\n",
       "          (word_embeddings): Embedding(30522, 128, padding_idx=0)\n",
       "          (position_embeddings): Embedding(512, 128)\n",
       "          (token_type_embeddings): Embedding(2, 128)\n",
       "          (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder): BertEncoder(\n",
       "          (layer): ModuleList(\n",
       "            (0-1): 2 x BertLayer(\n",
       "              (attention): BertAttention(\n",
       "                (self): BertSelfAttention(\n",
       "                  (query): Linear(\n",
       "                    in_features=128, out_features=128, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=128, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=128, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (key): Linear(\n",
       "                    in_features=128, out_features=128, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=128, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=128, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (value): Linear(\n",
       "                    in_features=128, out_features=128, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=128, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=128, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): BertSelfOutput(\n",
       "                  (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "                  (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): BertIntermediate(\n",
       "                (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): BertOutput(\n",
       "                (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "                (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (pooler): BertPooler(\n",
       "          (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (activation): Tanh()\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (classifier): ModulesToSaveWrapper(\n",
       "        (original_module): Linear(in_features=128, out_features=2, bias=True)\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): Linear(in_features=128, out_features=2, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained(model_name)\n",
    "config.num_labels = num_labels\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, config=config)\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS, \n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    target_modules = ['query', 'key', 'value'],\n",
    ")\n",
    "\n",
    "if peft_config is not None:\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    model.register_full_backward_hook(True)\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "errors = ModuleValidator.validate(model, strict=False)\n",
    "print(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = SGD(params=model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "privacy_engine = PrivacyEngine(accountant=\"rdp\")\n",
    "\n",
    "model, optimizer, train_dataloader = privacy_engine.make_private_with_epsilon(\n",
    "    module=model,\n",
    "    optimizer=optimizer,\n",
    "    data_loader=train_dataloader,\n",
    "    epochs=EPOCHS,\n",
    "    target_epsilon=EPSILON,\n",
    "    target_delta=DELTA,\n",
    "    max_grad_norm=MAX_GRAD_NORM,\n",
    "    batch_first=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Sigma = 0.479 | C = 0.5 | Initial DP (ε, δ) = (0, 0.0056179775280898875)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Using Sigma = {optimizer.noise_multiplier:.3f} | C = {optimizer.max_grad_norm} | Initial DP (ε, δ) = ({privacy_engine.get_epsilon(DELTA)}, {DELTA})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable Parameters: 12546 || All Parameters: 4398724 || Trainable Parameters (%): 0.29\n"
     ]
    }
   ],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"Trainable Parameters: {trainable_params} || All Parameters: {all_param} || Trainable Parameters (%): {100 * trainable_params / all_param:.2f}\"\n",
    "    )\n",
    "\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dataloader, optimizer, epoch, device):\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    losses = []\n",
    "    epsilon = []\n",
    "\n",
    "    with BatchMemoryManager(\n",
    "        data_loader=train_dataloader, \n",
    "        max_physical_batch_size=MAX_PHYSICAL_BATCH_SIZE,\n",
    "        optimizer=optimizer,\n",
    "        ) as memory_safe_data_loader:\n",
    "\n",
    "        for i, batch in tqdm(enumerate(memory_safe_data_loader), total=len(memory_safe_data_loader), desc=f\"Training Epoch: {epoch}\"):\n",
    "            \n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(**batch)\n",
    "            loss = criterion(outputs.logits, batch[\"labels\"])\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            if i % 8000 == 0:\n",
    "                epsilon = privacy_engine.get_epsilon(DELTA)\n",
    "\n",
    "                print(f\"Training Epoch: {epoch} | Loss: {np.mean(losses):.6f} | ε = {epsilon:.2f}\")                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_dataloader, device):\n",
    "    model.eval()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_dataloader, desc=\"Test\"):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "            outputs = model(**batch)\n",
    "            loss = criterion(outputs.logits, batch[\"labels\"])\n",
    "\n",
    "            preds = outputs.logits.argmax(dim=-1)\n",
    "            acc = accuracy_score(preds.cpu().numpy(), batch[\"labels\"].cpu().numpy())\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            accuracies.append(acc.item())\n",
    "\n",
    "    acc = np.mean(accuracies)\n",
    "    loss = np.mean(losses)\n",
    "\n",
    "    print(\n",
    "        f\"Test set: Loss: {loss:.4f}, Accuracy: {acc*100:.2f}%\"\n",
    "    )\n",
    "\n",
    "    return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddd8dc87bf9242a9bad442eac64a3cdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training 10 Epochs:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc6e6decf28d41e9bc20697df3f36aba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch: 1:   0%|          | 0/710 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1 | Loss: 0.699795 | ε = 0.00\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92f0ad1c97584d08b996179968153234",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch: 2:   0%|          | 0/710 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 2 | Loss: 0.669002 | ε = 3.48\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbec665d8c07401aad272648a4b15073",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch: 3:   0%|          | 0/710 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 3 | Loss: 0.947525 | ε = 4.28\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3b7ec4cf8e44b2f99ea0b2a56502f08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch: 4:   0%|          | 0/710 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 4 | Loss: 1.357647 | ε = 4.90\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a9b324800bc44008c316eed6b6b1bf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch: 5:   0%|          | 0/710 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 5 | Loss: 1.371178 | ε = 5.43\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20011e106484408ab9545d4065c33215",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch: 6:   0%|          | 0/710 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 6 | Loss: 1.437858 | ε = 5.94\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cf37a4ab9d74fe6b58dc3497d1f56fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch: 7:   0%|          | 0/710 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 7 | Loss: 1.320331 | ε = 6.39\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c17a50e86be64859a7a7beeb1a46c470",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch: 8:   0%|          | 0/710 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 8 | Loss: 1.497815 | ε = 6.82\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45de7ecc70ec414780c44a2d61c4e011",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch: 9:   0%|          | 0/710 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 9 | Loss: 1.427647 | ε = 7.25\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bc23e4cd7fa42eaa875f3cb23166d5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch: 10:   0%|          | 0/710 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 10 | Loss: 1.495963 | ε = 7.62\n"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(EPOCHS), desc=f'Training {EPOCHS} Epochs'):\n",
    "    train(model, train_dataloader, optimizer, epoch + 1, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final DP Guarantee (ε, δ)-DP = (7.99, 0.0056179775280898875)\n"
     ]
    }
   ],
   "source": [
    "final_epsilon = privacy_engine.get_epsilon(DELTA)\n",
    "print(f\"Final DP Guarantee (ε, δ)-DP = ({final_epsilon:.2f}, {DELTA})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e0b3d7c2c2f4f2da5da4a9cdd29b2ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Loss: 1.4513, Accuracy: 63.19%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.4513079524040222, 0.6319340440752634)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(model, test_dataloader, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
